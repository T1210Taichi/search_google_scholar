{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "738acc97-df59-4eff-8816-9d7d5bc10d98",
   "metadata": {},
   "source": [
    "# 目的\n",
    " 　requests, Beutiful Sourpなどのスクレイピングについて理解を深める。\n",
    " またgoogle scholarによる論文検索を行う。\n",
    " また上位5件タイトル、アブストラクト、URLをまとめる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e8857-f536-4b57-8b10-f88f621d36a2",
   "metadata": {},
   "source": [
    "# requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3645f5eb-2c84-4201-887a-3d9beec83830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ab7070-fcfd-4e40-a49c-bbacb1f1f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://scholar.google.co.jp/schhp?hl=ja\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d6f9a-b53d-4c65-bb29-25427763d353",
   "metadata": {},
   "source": [
    "## ステータスコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f1892dc-0dde-436e-999c-a209ed174660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b86248-11b2-4bc3-a9dd-ec4e2b3f365e",
   "metadata": {},
   "source": [
    "## レスポンスヘッダ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e31dfec-88a5-4fec-a648-26317eacbd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Wed, 31 Aug 2022 08:33:22 GMT', 'Expires': 'Wed, 31 Aug 2022 08:33:22 GMT', 'Cache-Control': 'private, max-age=0', 'Content-Type': 'text/html; charset=Shift_JIS', 'P3P': 'CP=\"This is not a P3P policy! See g.co/p3phelp for more info.\"', 'X-Content-Type-Options': 'nosniff', 'Content-Encoding': 'gzip', 'Server': 'scholar', 'X-XSS-Protection': '0', 'X-Frame-Options': 'SAMEORIGIN', 'Set-Cookie': 'NID=511=P4Zn2d7tm6cTuQpPluHlaNI6SeApnYX1Cbn4h9lSccrcpNvgMBjO_bDF6k-AVQUjynW3oO32pZlGpdELZw5oZ3Qg7oBIIv33h4Sop9vnnBLjzQZz3Aj7iBSux3NBgyxIObOE9jZfq5KsAgxBrZXS78Bp3EQrdaDJcPYcezorPhk; expires=Thu, 02-Mar-2023 08:33:22 GMT; path=/; domain=.google.co.jp; HttpOnly, GSP=LM=1661934802:S=voBSpnAkHzmOhKy0; expires=Fri, 30-Aug-2024 08:33:22 GMT; path=/; domain=scholar.google.co.jp; HttpOnly', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"', 'Transfer-Encoding': 'chunked'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebb966ee-f47c-4fe7-a61e-f50608cc0aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date \t Wed, 31 Aug 2022 08:33:22 GMT\n",
      "Expires \t Wed, 31 Aug 2022 08:33:22 GMT\n",
      "Cache-Control \t private, max-age=0\n",
      "Content-Type \t text/html; charset=Shift_JIS\n",
      "P3P \t CP=\"This is not a P3P policy! See g.co/p3phelp for more info.\"\n",
      "X-Content-Type-Options \t nosniff\n",
      "Content-Encoding \t gzip\n",
      "Server \t scholar\n",
      "X-XSS-Protection \t 0\n",
      "X-Frame-Options \t SAMEORIGIN\n",
      "Set-Cookie \t NID=511=P4Zn2d7tm6cTuQpPluHlaNI6SeApnYX1Cbn4h9lSccrcpNvgMBjO_bDF6k-AVQUjynW3oO32pZlGpdELZw5oZ3Qg7oBIIv33h4Sop9vnnBLjzQZz3Aj7iBSux3NBgyxIObOE9jZfq5KsAgxBrZXS78Bp3EQrdaDJcPYcezorPhk; expires=Thu, 02-Mar-2023 08:33:22 GMT; path=/; domain=.google.co.jp; HttpOnly, GSP=LM=1661934802:S=voBSpnAkHzmOhKy0; expires=Fri, 30-Aug-2024 08:33:22 GMT; path=/; domain=scholar.google.co.jp; HttpOnly\n",
      "Alt-Svc \t h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"\n",
      "Transfer-Encoding \t chunked\n"
     ]
    }
   ],
   "source": [
    "for key, value in response.headers.items():\n",
    "    print(key,\"\\t\",value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5425e-4e20-4792-93a1-d35edc8c687a",
   "metadata": {},
   "source": [
    "## エンコーディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "424029bb-1ee8-4bca-8503-c348799e4808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shift_JIS'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7883ca-0f02-46b7-99bd-a65237aa2de3",
   "metadata": {},
   "source": [
    "## レスポンスのバイナリデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bb69748-df5f-41f8-b92e-e6310b07d3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!doctype html><html><head><title>Google Scholar</title><meta http-equiv=\"Content-Type\" content=\"text/html;charset=Shift_JIS\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"><meta name=\"referrer\" content=\"always\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1,minimum-scale=1,maximum-scale=2\"><meta name=\"format-detection\" content=\"telephone=no\"><link rel=\"shortcut icon\" href=\"/favicon.ico\"><meta name=\"google-site-verification\" content=\"Y8J1_s45IeTudoUMT5t7AB5kel7unVEK-WjxX'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8144f4d-4b47-43c0-a57f-15a889e148b8",
   "metadata": {},
   "source": [
    "## レスポンスのテキストデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4645ced-befe-4d8e-992a-276193f5f910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html><html><head><title>Google Scholar</title><meta http-equiv=\"Content-Type\" content=\"text/html;charset=Shift_JIS\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"><meta name=\"referrer\" content=\"always\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1,minimum-scale=1,maximum-scale=2\"><meta name=\"format-detection\" content=\"telephone=no\"><link rel=\"shortcut icon\" href=\"/favicon.ico\"><meta name=\"google-site-verification\" content=\"Y8J1_s45IeTudoUMT5t7AB5kel7unVEK-WjxX'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768285e-ab79-498b-8064-07dba431ebf6",
   "metadata": {},
   "source": [
    "## レスポンスのcookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba9a2deb-fc75-458b-92d4-53ba0d56f6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RequestsCookieJar[Cookie(version=0, name='NID', value='511=P4Zn2d7tm6cTuQpPluHlaNI6SeApnYX1Cbn4h9lSccrcpNvgMBjO_bDF6k-AVQUjynW3oO32pZlGpdELZw5oZ3Qg7oBIIv33h4Sop9vnnBLjzQZz3Aj7iBSux3NBgyxIObOE9jZfq5KsAgxBrZXS78Bp3EQrdaDJcPYcezorPhk', port=None, port_specified=False, domain='.google.co.jp', domain_specified=True, domain_initial_dot=True, path='/', path_specified=True, secure=False, expires=1677746002, discard=False, comment=None, comment_url=None, rest={'HttpOnly': None}, rfc2109=False), Cookie(version=0, name='GSP', value='LM=1661934802:S=voBSpnAkHzmOhKy0', port=None, port_specified=False, domain='.scholar.google.co.jp', domain_specified=True, domain_initial_dot=False, path='/', path_specified=True, secure=False, expires=1725006802, discard=False, comment=None, comment_url=None, rest={'HttpOnly': None}, rfc2109=False)]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.cookies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16470ee5-1cdc-42aa-b421-13ab12214a3a",
   "metadata": {},
   "source": [
    "# google scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552d8c21-101c-4d49-87a2-1e789d62ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://scholar.google.co.jp/scholar\"\n",
    "param = {\"q\" : \"data augmentation nmt\"}\n",
    "response_param = requests.get(url,params=param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe133050-b959-48c3-a6c1-87bd27627323",
   "metadata": {},
   "source": [
    "## beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc41b3a6-aeef-4cc1-8c0b-5897bcc3d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6e27052-0864-4f46-8460-f4f9411d0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#htmlとして解析\n",
    "bs = BeautifulSoup(response_param.text,\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ade4d1-3af3-4a20-beaf-2a0aff8c08d2",
   "metadata": {},
   "source": [
    "### タイトルを抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ed6561c-3fb4-4032-bcac-560f81559119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#タグを抽出\n",
    "h3_tags = bs.find_all(\"h3\",class_ = \"gs_rt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "548574db-cb98-40f3-9be3-3177f892bee5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Soft contextual data augmentation for neural machine translation\n",
      "1 Data augmentation using back-translation for context-aware neural machine translation\n",
      "2 Improving Data Augmentation for Low-Resource NMT Guided by POS-Tagging and Paraphrase Embedding\n",
      "3 SwitchOut: an efficient data augmentation algorithm for neural machine translation\n",
      "4 Benefits of data augmentation for NMT-based text normalization of user-generated content\n",
      "5 Data augmentation for low‐resource languages NMT guided by constrained sampling\n",
      "6 Dictionary-based data augmentation for cross-domain neural machine translation\n",
      "7 Soft contextual data augmentation for neural machine translation\n",
      "8 Multi-source neural machine translation with data augmentation\n",
      "9 Improving neural machine translation robustness via data augmentation: Beyond back translation\n"
     ]
    }
   ],
   "source": [
    "title = []\n",
    "for i,h3 in enumerate(h3_tags):\n",
    "    a_tag = h3.find(\"a\")\n",
    "    print(i,a_tag.get_text())\n",
    "    title.append(a_tag.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a26c0a6-8494-4220-8199-a806065d33af",
   "metadata": {},
   "source": [
    "### pdfのURLを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee265b78-336d-400e-b559-14b8ba9294b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#タグを抽出\n",
    "pdf_a_tags = bs.find_all(class_=\"gs_ggsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5806def-1f7b-4591-9f8a-2cfa7403dc95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 https://aclanthology.org/P19-1555.pdf\n",
      "1 https://aclanthology.org/D19-6504.pdf\n",
      "2 https://www.researchgate.net/profile/Miradeljan-Muhammad/publication/351307805_Improving_the_Data_Augmentation_for_Low-Resource_NMT_Guided_by_POS-Tagging_and_Paraphrase_Embedding/links/6127bd2338818c2eaf629195/Improving-the-Data-Augmentation-for-Low-Resource-NMT-Guided-by-POS-Tagging-and-Paraphrase-Embedding.pdf\n",
      "3 https://arxiv.org/pdf/1808.07512\n",
      "4 https://biblio.ugent.be/publication/8629139/file/8663091.pdf\n",
      "5 http://www.miradeljan.com/files/ijis2021_preprint.pdf\n",
      "6 https://arxiv.org/pdf/2004.02577\n",
      "7 https://arxiv.org/pdf/1905.10523\n",
      "8 https://arxiv.org/pdf/1810.06826\n",
      "9 https://arxiv.org/pdf/1910.03009\n"
     ]
    }
   ],
   "source": [
    "for i,a_tag in enumerate(pdf_a_tags):\n",
    "    for a in a_tag.select(\"a\"):\n",
    "        #hrefを取得\n",
    "        print(i,a.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64edd391-85d7-41c4-8071-624fa593020d",
   "metadata": {},
   "source": [
    "## サイトのURLを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8627fa25-f144-4e2f-ae39-462f112843ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#タグ抽出\n",
    "url_a_tags = bs.find_all(\"h3\",class_=\"gs_rt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "832a4edd-7256-4a56-8b99-b62e75c73efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 https://aclanthology.org/P19-1555/?ref=https://githubhelp.com\n",
      "1 https://aclanthology.org/D19-6504/?ref=https://githubhelp.com\n",
      "2 https://dl.acm.org/doi/abs/10.1145/3464427\n",
      "3 https://arxiv.org/abs/1808.07512\n",
      "4 https://biblio.ugent.be/publication/8629139\n",
      "5 https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22616\n",
      "6 https://arxiv.org/abs/2004.02577\n",
      "7 https://arxiv.org/abs/1905.10523\n",
      "8 https://arxiv.org/abs/1810.06826\n",
      "9 https://arxiv.org/abs/1910.03009\n"
     ]
    }
   ],
   "source": [
    "URL = []\n",
    "for i, a_tag in enumerate(url_a_tags):\n",
    "    for a in a_tag.select(\"a\"):\n",
    "        print(i,a.get(\"href\"))\n",
    "        URL.append(a.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa46465-e131-4213-b0d1-b1536886ca5a",
   "metadata": {},
   "source": [
    "## get abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d3171911-dd47-4ee9-8d47-c1a25afd9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from requests.exceptions import Timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c396d955-6e71-4950-ad50-dfc90ff1c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstract(url):\n",
    "    \"\"\"get_abstract\n",
    "    URLを与えて、そのページのabstractを返す\n",
    "    \n",
    "    params:\n",
    "        url : str\n",
    "            検索したいurl\n",
    "    \n",
    "    return:\n",
    "        abstract : str\n",
    "            その論文のabstract\n",
    "    \"\"\"\n",
    "    \n",
    "    ##requests\n",
    "    count = 0\n",
    "    while count < 5:\n",
    "        try:\n",
    "            header={\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\"}\n",
    "            response = requests.get(url,timeout=(3.0,7.5))\n",
    "        except Timeout:\n",
    "            abstract = \"Nothing\"\n",
    "            break\n",
    "            \n",
    "        if response.status_code == 200:\n",
    "            break\n",
    "        print(url,response.status_code)\n",
    "        count += 1\n",
    "        \n",
    "    ##beutifulsoup\n",
    "    #htmlとして解析\n",
    "    bs = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "    abstract = \"\"\n",
    "    #サイトによって変える\n",
    "    if \"biblio.ugent.be\" in url:\n",
    "        #abstractを取得\n",
    "        #pattern = \"subtle roomy pbottom-default mbottom-default border-bottom\"\n",
    "        abstract_tags = bs.find_all(\"dd\",itemprop=\"description\")\n",
    "        abstract = abstract_tags[0].get_text()\n",
    "    elif \"aclanthology.org\" in url:\n",
    "        abstract_tags = bs.find_all(class_=\"card-body acl-abstract\")\n",
    "        abstract = abstract_tags[0].find(\"span\").get_text()\n",
    "    elif \"dl.acm.org\" in url:\n",
    "        abstract_tags = bs.find_all(class_=\"abstractSection abstractInFull\")\n",
    "        abstract = abstract_tags[0].find(\"p\").get_text()\n",
    "    elif \"arxiv.org\" in url:\n",
    "        abstract_tags = bs.find_all(class_=\"abstract mathjax\")\n",
    "        abstract = abstract_tags[0].get_text()[12:]\n",
    "    elif \"onlinelibrary\" in url:\n",
    "        #abstract_tags = bs.select(\"[class='article-section__content en main']\")\n",
    "        #print(response.status_code)\n",
    "        #abstract = abstract_tags[0].find(\"p\").get_text()\n",
    "        abstract = \"nothing\"\n",
    "        \n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "650210ed-ceda-464c-adac-c8fea172a007",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 https://aclanthology.org/P19-1555/?ref=https://githubhelp.com\n",
      "0 While data augmentation is an important trick to boost the accuracy of deep learning methods in comp\n",
      "1 https://aclanthology.org/D19-6504/?ref=https://githubhelp.com\n",
      "1 A single sentence does not always convey information that is enough to translate it into other langu\n",
      "2 https://dl.acm.org/doi/abs/10.1145/3464427\n",
      "2 Data augmentation is an approach for several text generation tasks. Generally, in the machine transl\n",
      "3 https://arxiv.org/abs/1808.07512\n",
      "3 In this work, we examine methods for data augmentation for text-based tasks\n",
      "such as neural machine t\n",
      "4 https://biblio.ugent.be/publication/8629139\n",
      "4 One of the most persistent characteristics of written user-generated content (UGC) is the use of non\n",
      "5 https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22616\n",
      "https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22616 503\n",
      "https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22616 503\n",
      "https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22616 503\n",
      "https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22616 503\n",
      "https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22616 503\n",
      "5 nothing\n",
      "6 https://arxiv.org/abs/2004.02577\n",
      "6 Existing data augmentation approaches for neural machine translation (NMT)\n",
      "have predominantly relied\n",
      "7 https://arxiv.org/abs/1905.10523\n",
      "7 While data augmentation is an important trick to boost the accuracy of deep\n",
      "learning methods in comp\n",
      "8 https://arxiv.org/abs/1810.06826\n",
      "8 Multi-source translation systems translate from multiple languages to a\n",
      "single target language. By u\n",
      "9 https://arxiv.org/abs/1910.03009\n",
      "9 Neural Machine Translation (NMT) models have been proved strong when\n",
      "translating clean texts, but th\n"
     ]
    }
   ],
   "source": [
    "abstracts = []\n",
    "for i, a_tag in enumerate(url_a_tags):\n",
    "    for a in a_tag.select(\"a\"):\n",
    "        print(i,a.get(\"href\"))\n",
    "        url = a.get(\"href\")\n",
    "        abstract = get_abstract(url)\n",
    "        print(i,abstract[:100])\n",
    "        abstracts.append(abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c25341d-e804-4083-b350-827f00f42a19",
   "metadata": {},
   "source": [
    "## dataframeにまとめる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "faac3586-ba50-49f8-b905-9563545113ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "62c58804-18cf-463c-a79b-031677edb35b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_fb430\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fb430_level0_col0\" class=\"col_heading level0 col0\" >title</th>\n",
       "      <th id=\"T_fb430_level0_col1\" class=\"col_heading level0 col1\" >abstract</th>\n",
       "      <th id=\"T_fb430_level0_col2\" class=\"col_heading level0 col2\" >URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fb430_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_fb430_row0_col0\" class=\"data row0 col0\" >Soft contextual data augmentation for neural machine translation</td>\n",
       "      <td id=\"T_fb430_row0_col1\" class=\"data row0 col1\" >While data augmentation is an important trick to boost the accuracy of deep learning methods in computer vision tasks, its study in natural language tasks is still very limited. In this paper, we present a novel data augmentation method for neural machine translation.Different from previous augmentation methods that randomly drop, swap or replace words with other words in a sentence, we softly augment a randomly chosen word in a sentence by its contextual mixture of multiple related words. More accurately, we replace the one-hot representation of a word by a distribution (provided by a language model) over the vocabulary, i.e., replacing the embedding of this word by a weighted combination of multiple semantically similar words. Since the weights of those words depend on the contextual information of the word to be replaced,the newly generated sentences capture much richer information than previous augmentation methods. Experimental results on both small scale and large scale machine translation data sets demonstrate the superiority of our method over strong baselines.</td>\n",
       "      <td id=\"T_fb430_row0_col2\" class=\"data row0 col2\" ><a href=\"https://aclanthology.org/P19-1555/?ref=https://githubhelp.com\" target=\"_blank\">https://aclanthology.org/P19-1555/?ref=https://githubhelp.com</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb430_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_fb430_row1_col0\" class=\"data row1 col0\" >Data augmentation using back-translation for context-aware neural machine translation</td>\n",
       "      <td id=\"T_fb430_row1_col1\" class=\"data row1 col1\" >A single sentence does not always convey information that is enough to translate it into other languages. Some target languages need to add or specialize words that are omitted or ambiguous in the source languages (e.g, zero pronouns in translating Japanese to English or epicene pronouns in translating English to French). To translate such ambiguous sentences, we need contexts beyond a single sentence, and have so far explored context-aware neural machine translation (NMT). However, a large amount of parallel corpora is not easily available to train accurate context-aware NMT models. In this study, we first obtain large-scale pseudo parallel corpora by back-translating monolingual data, and then investigate its impact on the translation accuracy of context-aware NMT models. We evaluated context-aware NMT models trained with small parallel corpora and the large-scale pseudo parallel corpora on English-Japanese and English-French datasets to demonstrate the large impact of the data augmentation for context-aware NMT models.</td>\n",
       "      <td id=\"T_fb430_row1_col2\" class=\"data row1 col2\" ><a href=\"https://aclanthology.org/D19-6504/?ref=https://githubhelp.com\" target=\"_blank\">https://aclanthology.org/D19-6504/?ref=https://githubhelp.com</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb430_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_fb430_row2_col0\" class=\"data row2 col0\" >Improving Data Augmentation for Low-Resource NMT Guided by POS-Tagging and Paraphrase Embedding</td>\n",
       "      <td id=\"T_fb430_row2_col1\" class=\"data row2 col1\" >Data augmentation is an approach for several text generation tasks. Generally, in the machine translation paradigm, mainly in low-resource language scenarios, many data augmentation methods have been proposed. The most used approaches for generating pseudo data mainly lay in word omission, random sampling, or replacing some words in the text. However, previous methods barely guarantee the quality of augmented data. In this work, we try to build the data by using paraphrase embedding and POS-Tagging. Namely, we generate the fake monolingual corpus by replacing the main four POS-Tagging labels, such as noun, adjective, adverb, and verb, based on both the paraphrase table and their similarity. We select the bigger corpus size of the paraphrase table with word level and obtain the word embedding of each word in the table, then calculate the cosine similarity between these words and tagged words in the original sequence. In addition, we exploit the ranking algorithm to choose highly similar words to reduce semantic errors and leverage the POS-Tagging replacement to mitigate syntactic error to some extent. Experimental results show that our augmentation method consistently outperforms all previous SOTA methods on the low-resource language pairs in seven language pairs from four corpora by 1.16 to 2.39 BLEU points.</td>\n",
       "      <td id=\"T_fb430_row2_col2\" class=\"data row2 col2\" ><a href=\"https://dl.acm.org/doi/abs/10.1145/3464427\" target=\"_blank\">https://dl.acm.org/doi/abs/10.1145/3464427</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb430_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_fb430_row3_col0\" class=\"data row3 col0\" >SwitchOut: an efficient data augmentation algorithm for neural machine translation</td>\n",
       "      <td id=\"T_fb430_row3_col1\" class=\"data row3 col1\" >In this work, we examine methods for data augmentation for text-based tasks\n",
       "such as neural machine translation (NMT). We formulate the design of a data\n",
       "augmentation policy with desirable properties as an optimization problem, and\n",
       "derive a generic analytic solution. This solution not only subsumes some\n",
       "existing augmentation schemes, but also leads to an extremely simple data\n",
       "augmentation strategy for NMT: randomly replacing words in both the source\n",
       "sentence and the target sentence with other random words from their\n",
       "corresponding vocabularies. We name this method SwitchOut. Experiments on three\n",
       "translation datasets of different scales show that SwitchOut yields consistent\n",
       "improvements of about 0.5 BLEU, achieving better or comparable performances to\n",
       "strong alternatives such as word dropout (Sennrich et al., 2016a). Code to\n",
       "implement this method is included in the appendix.\n",
       "\n",
       "    </td>\n",
       "      <td id=\"T_fb430_row3_col2\" class=\"data row3 col2\" ><a href=\"https://arxiv.org/abs/1808.07512\" target=\"_blank\">https://arxiv.org/abs/1808.07512</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb430_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_fb430_row4_col0\" class=\"data row4 col0\" >Benefits of data augmentation for NMT-based text normalization of user-generated content</td>\n",
       "      <td id=\"T_fb430_row4_col1\" class=\"data row4 col1\" >One of the most persistent characteristics of written user-generated content (UGC) is the use of non-standard words. This characteristic contributes to an increased difficulty to automatically process and analyze UGC. Text normalization is the task of transforming lexical variants to their canonical forms and is often used as a pre-processing step for conventional NLP tasks in order to overcome the performance drop that NLP systems experience when applied to UGC. In this work, we follow a Neural Machine Translation approach to text normalization. To train such an encoder-decoder model, large parallel training corpora of sentence pairs are required. However, obtaining large data sets with UGC and their normalized version is not trivial, especially for languages other than English. In this paper, we explore how to overcome this data bottleneck for Dutch, a low-resource language. We start off with a publicly available tiny parallel Dutch data set comprising three UGC genres and compare two different approaches. The first is to manually normalize and add training data, a money and time-consuming task. The second approach is a set of data augmentation techniques which increase data size by converting existing resources into synthesized non-standard forms. Our results reveal that a combination of both approaches leads to the best results.</td>\n",
       "      <td id=\"T_fb430_row4_col2\" class=\"data row4 col2\" ><a href=\"https://biblio.ugent.be/publication/8629139\" target=\"_blank\">https://biblio.ugent.be/publication/8629139</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb430_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_fb430_row5_col0\" class=\"data row5 col0\" >Data augmentation for low‐resource languages NMT guided by constrained sampling</td>\n",
       "      <td id=\"T_fb430_row5_col1\" class=\"data row5 col1\" >nothing</td>\n",
       "      <td id=\"T_fb430_row5_col2\" class=\"data row5 col2\" ><a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22616\" target=\"_blank\">https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22616</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb430_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_fb430_row6_col0\" class=\"data row6 col0\" >Dictionary-based data augmentation for cross-domain neural machine translation</td>\n",
       "      <td id=\"T_fb430_row6_col1\" class=\"data row6 col1\" >Existing data augmentation approaches for neural machine translation (NMT)\n",
       "have predominantly relied on back-translating in-domain (IND) monolingual\n",
       "corpora. These methods suffer from issues associated with a domain information\n",
       "gap, which leads to translation errors for low frequency and out-of-vocabulary\n",
       "terminology. This paper proposes a dictionary-based data augmentation (DDA)\n",
       "method for cross-domain NMT. DDA synthesizes a domain-specific dictionary with\n",
       "general domain corpora to automatically generate a large-scale pseudo-IND\n",
       "parallel corpus. The generated pseudo-IND data can be used to enhance a general\n",
       "domain trained baseline. The experiments show that the DDA-enhanced NMT models\n",
       "demonstrate consistent significant improvements, outperforming the baseline\n",
       "models by 3.75-11.53 BLEU. The proposed method is also able to further improve\n",
       "the performance of the back-translation based and IND-finetuned NMT models. The\n",
       "improvement is associated with the enhanced domain coverage produced by DDA.\n",
       "\n",
       "    </td>\n",
       "      <td id=\"T_fb430_row6_col2\" class=\"data row6 col2\" ><a href=\"https://arxiv.org/abs/2004.02577\" target=\"_blank\">https://arxiv.org/abs/2004.02577</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb430_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_fb430_row7_col0\" class=\"data row7 col0\" >Soft contextual data augmentation for neural machine translation</td>\n",
       "      <td id=\"T_fb430_row7_col1\" class=\"data row7 col1\" >While data augmentation is an important trick to boost the accuracy of deep\n",
       "learning methods in computer vision tasks, its study in natural language tasks\n",
       "is still very limited. In this paper, we present a novel data augmentation\n",
       "method for neural machine translation. Different from previous augmentation\n",
       "methods that randomly drop, swap or replace words with other words in a\n",
       "sentence, we softly augment a randomly chosen word in a sentence by its\n",
       "contextual mixture of multiple related words. More accurately, we replace the\n",
       "one-hot representation of a word by a distribution (provided by a language\n",
       "model) over the vocabulary, i.e., replacing the embedding of this word by a\n",
       "weighted combination of multiple semantically similar words. Since the weights\n",
       "of those words depend on the contextual information of the word to be replaced,\n",
       "the newly generated sentences capture much richer information than previous\n",
       "augmentation methods. Experimental results on both small scale and large scale\n",
       "machine translation datasets demonstrate the superiority of our method over\n",
       "strong baselines.\n",
       "\n",
       "    </td>\n",
       "      <td id=\"T_fb430_row7_col2\" class=\"data row7 col2\" ><a href=\"https://arxiv.org/abs/1905.10523\" target=\"_blank\">https://arxiv.org/abs/1905.10523</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb430_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_fb430_row8_col0\" class=\"data row8 col0\" >Multi-source neural machine translation with data augmentation</td>\n",
       "      <td id=\"T_fb430_row8_col1\" class=\"data row8 col1\" >Multi-source translation systems translate from multiple languages to a\n",
       "single target language. By using information from these multiple sources, these\n",
       "systems achieve large gains in accuracy. To train these systems, it is\n",
       "necessary to have corpora with parallel text in multiple sources and the target\n",
       "language. However, these corpora are rarely complete in practice due to the\n",
       "difficulty of providing human translations in all of the relevant languages. In\n",
       "this paper, we propose a data augmentation approach to fill such incomplete\n",
       "parts using multi-source neural machine translation (NMT). In our experiments,\n",
       "results varied over different language combinations but significant gains were\n",
       "observed when using a source language similar to the target language.\n",
       "\n",
       "    </td>\n",
       "      <td id=\"T_fb430_row8_col2\" class=\"data row8 col2\" ><a href=\"https://arxiv.org/abs/1810.06826\" target=\"_blank\">https://arxiv.org/abs/1810.06826</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb430_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_fb430_row9_col0\" class=\"data row9 col0\" >Improving neural machine translation robustness via data augmentation: Beyond back translation</td>\n",
       "      <td id=\"T_fb430_row9_col1\" class=\"data row9 col1\" >Neural Machine Translation (NMT) models have been proved strong when\n",
       "translating clean texts, but they are very sensitive to noise in the input.\n",
       "Improving NMT models robustness can be seen as a form of \"domain\" adaption to\n",
       "noise. The recently created Machine Translation on Noisy Text task corpus\n",
       "provides noisy-clean parallel data for a few language pairs, but this data is\n",
       "very limited in size and diversity. The state-of-the-art approaches are heavily\n",
       "dependent on large volumes of back-translated data. This paper has two main\n",
       "contributions: Firstly, we propose new data augmentation methods to extend\n",
       "limited noisy data and further improve NMT robustness to noise while keeping\n",
       "the models small. Secondly, we explore the effect of utilizing noise from\n",
       "external data in the form of speech transcripts and show that it could help\n",
       "robustness.\n",
       "\n",
       "    </td>\n",
       "      <td id=\"T_fb430_row9_col2\" class=\"data row9 col2\" ><a href=\"https://arxiv.org/abs/1910.03009\" target=\"_blank\">https://arxiv.org/abs/1910.03009</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19e81370ee0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"title\"] = title\n",
    "df[\"abstract\"] = abstracts\n",
    "df[\"URL\"] = URL\n",
    "\n",
    "df.style.format(formatter={\"URL\" : make_clickable})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad68fe3f-f667-4875-a757-2a9c11ef20c9",
   "metadata": {},
   "source": [
    "# まとめ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4e6d9fb6-a0c2-4f94-ada6-f00c3ed4878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from requests.exceptions import Timeout\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "af793b73-0b64-4820-b265-577150542299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstract(url):\n",
    "    \"\"\"get_abstract\n",
    "    URLを与えて、そのページのabstractを返す\n",
    "    \n",
    "    params:\n",
    "        url : str\n",
    "            検索したいurl\n",
    "    \n",
    "    return:\n",
    "        abstract : str\n",
    "            その論文のabstract\n",
    "    \"\"\"\n",
    "    \n",
    "    ##requests\n",
    "    count = 0\n",
    "    while count < 5:\n",
    "        try:\n",
    "            response = requests.get(url,timeout=(3.0,7.5))\n",
    "        except Timeout:\n",
    "            #abstract = \"Nothing\"\n",
    "            continue\n",
    "            \n",
    "        if response.status_code == 200:\n",
    "            break\n",
    "        count += 1\n",
    "        \n",
    "    ##beutifulsoup\n",
    "    #htmlとして解析\n",
    "    bs = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "    abstract = \"\"\n",
    "    #サイトによって変える\n",
    "    if \"biblio.ugent.be\" in url:\n",
    "        #abstractを取得\n",
    "        #pattern = \"subtle roomy pbottom-default mbottom-default border-bottom\"\n",
    "        abstract_tags = bs.find_all(\"dd\",itemprop=\"description\")\n",
    "        abstract = abstract_tags[0].get_text()\n",
    "    elif \"aclanthology.org\" in url:\n",
    "        abstract_tags = bs.find_all(class_=\"card-body acl-abstract\")\n",
    "        abstract = abstract_tags[0].find(\"span\").get_text()\n",
    "    elif \"dl.acm.org\" in url:\n",
    "        abstract_tags = bs.find_all(class_=\"abstractSection abstractInFull\")\n",
    "        abstract = abstract_tags[0].find(\"p\").get_text()\n",
    "    elif \"arxiv.org\" in url:\n",
    "        abstract_tags = bs.find_all(class_=\"abstract mathjax\")\n",
    "        abstract = abstract_tags[0].get_text()[12:]\n",
    "    elif \"onlinelibrary\" in url:\n",
    "        #abstract_tags = bs.select(\"[class='article-section__content en main']\")\n",
    "        #print(response.status_code)\n",
    "        #abstract = abstract_tags[0].find(\"p\").get_text()\n",
    "        abstract = \"nothing\"\n",
    "        \n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0eabe4b5-9be0-4bbe-839f-5026000e67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clickable(val):\n",
    "    return '<a href=\"{}\" target=\"_blank\">{}</a>'.format(val,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ccfc97a2-5acd-409e-bc2c-93e0459ffc95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search_google_schalor(keywords):\n",
    "    \"\"\"キーワードを与え、そのキーワードでgoogle schalorで検索、上位10件の論文のタイトル、アブストラクト、URLを表にまとめる\n",
    "    \"\"\"\n",
    "    \n",
    "    #requests\n",
    "    url = \"https://scholar.google.co.jp/scholar\"\n",
    "    param = {\"q\" : keywords}\n",
    "    response_param = requests.get(url,params=param)\n",
    "    \n",
    "    #タイトル抽出\n",
    "    title_tags = bs.find_all(\"h3\",class_ = \"gs_rt\")\n",
    "    titles = []\n",
    "    for h3 in title_tags:\n",
    "        a_tag = h3.find(\"a\")\n",
    "        titles.append(a_tag.get_text())\n",
    "    #URLを抽出\n",
    "    pdf_a_tags = bs.find_all(class_=\"gs_ggsd\")    \n",
    "    #url, abstractを抽出\n",
    "    abstracts = []\n",
    "    urls = []\n",
    "    for a_tag in url_a_tags:\n",
    "        for a in a_tag.select(\"a\"):\n",
    "            url = a.get(\"href\")\n",
    "            urls.append(url)\n",
    "            abstract = get_abstract(url)\n",
    "            abstracts.append(abstract)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"title\"] = titles\n",
    "    df[\"abstract\"] = abstracts\n",
    "    df[\"URL\"] = urls\n",
    "    df = df.style.format(formatter={\"URL\" : make_clickable})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "35fb7511-b351-44ef-b7da-fc2088640cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    keywords = input(\"検索文を入力してください\")\n",
    "    df = search_google_schalor(keywords)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d2058eeb-be1f-4398-bcdc-3b48d494b8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "検索文を入力してください nlp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e5f14\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e5f14_level0_col0\" class=\"col_heading level0 col0\" >title</th>\n",
       "      <th id=\"T_e5f14_level0_col1\" class=\"col_heading level0 col1\" >abstract</th>\n",
       "      <th id=\"T_e5f14_level0_col2\" class=\"col_heading level0 col2\" >URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f14_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e5f14_row0_col0\" class=\"data row0 col0\" >Soft contextual data augmentation for neural machine translation</td>\n",
       "      <td id=\"T_e5f14_row0_col1\" class=\"data row0 col1\" >While data augmentation is an important trick to boost the accuracy of deep learning methods in computer vision tasks, its study in natural language tasks is still very limited. In this paper, we present a novel data augmentation method for neural machine translation.Different from previous augmentation methods that randomly drop, swap or replace words with other words in a sentence, we softly augment a randomly chosen word in a sentence by its contextual mixture of multiple related words. More accurately, we replace the one-hot representation of a word by a distribution (provided by a language model) over the vocabulary, i.e., replacing the embedding of this word by a weighted combination of multiple semantically similar words. Since the weights of those words depend on the contextual information of the word to be replaced,the newly generated sentences capture much richer information than previous augmentation methods. Experimental results on both small scale and large scale machine translation data sets demonstrate the superiority of our method over strong baselines.</td>\n",
       "      <td id=\"T_e5f14_row0_col2\" class=\"data row0 col2\" ><a href=\"https://aclanthology.org/P19-1555/?ref=https://githubhelp.com\" target=\"_blank\">https://aclanthology.org/P19-1555/?ref=https://githubhelp.com</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f14_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e5f14_row1_col0\" class=\"data row1 col0\" >Data augmentation using back-translation for context-aware neural machine translation</td>\n",
       "      <td id=\"T_e5f14_row1_col1\" class=\"data row1 col1\" >A single sentence does not always convey information that is enough to translate it into other languages. Some target languages need to add or specialize words that are omitted or ambiguous in the source languages (e.g, zero pronouns in translating Japanese to English or epicene pronouns in translating English to French). To translate such ambiguous sentences, we need contexts beyond a single sentence, and have so far explored context-aware neural machine translation (NMT). However, a large amount of parallel corpora is not easily available to train accurate context-aware NMT models. In this study, we first obtain large-scale pseudo parallel corpora by back-translating monolingual data, and then investigate its impact on the translation accuracy of context-aware NMT models. We evaluated context-aware NMT models trained with small parallel corpora and the large-scale pseudo parallel corpora on English-Japanese and English-French datasets to demonstrate the large impact of the data augmentation for context-aware NMT models.</td>\n",
       "      <td id=\"T_e5f14_row1_col2\" class=\"data row1 col2\" ><a href=\"https://aclanthology.org/D19-6504/?ref=https://githubhelp.com\" target=\"_blank\">https://aclanthology.org/D19-6504/?ref=https://githubhelp.com</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f14_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e5f14_row2_col0\" class=\"data row2 col0\" >Improving Data Augmentation for Low-Resource NMT Guided by POS-Tagging and Paraphrase Embedding</td>\n",
       "      <td id=\"T_e5f14_row2_col1\" class=\"data row2 col1\" >Data augmentation is an approach for several text generation tasks. Generally, in the machine translation paradigm, mainly in low-resource language scenarios, many data augmentation methods have been proposed. The most used approaches for generating pseudo data mainly lay in word omission, random sampling, or replacing some words in the text. However, previous methods barely guarantee the quality of augmented data. In this work, we try to build the data by using paraphrase embedding and POS-Tagging. Namely, we generate the fake monolingual corpus by replacing the main four POS-Tagging labels, such as noun, adjective, adverb, and verb, based on both the paraphrase table and their similarity. We select the bigger corpus size of the paraphrase table with word level and obtain the word embedding of each word in the table, then calculate the cosine similarity between these words and tagged words in the original sequence. In addition, we exploit the ranking algorithm to choose highly similar words to reduce semantic errors and leverage the POS-Tagging replacement to mitigate syntactic error to some extent. Experimental results show that our augmentation method consistently outperforms all previous SOTA methods on the low-resource language pairs in seven language pairs from four corpora by 1.16 to 2.39 BLEU points.</td>\n",
       "      <td id=\"T_e5f14_row2_col2\" class=\"data row2 col2\" ><a href=\"https://dl.acm.org/doi/abs/10.1145/3464427\" target=\"_blank\">https://dl.acm.org/doi/abs/10.1145/3464427</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f14_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e5f14_row3_col0\" class=\"data row3 col0\" >SwitchOut: an efficient data augmentation algorithm for neural machine translation</td>\n",
       "      <td id=\"T_e5f14_row3_col1\" class=\"data row3 col1\" >In this work, we examine methods for data augmentation for text-based tasks\n",
       "such as neural machine translation (NMT). We formulate the design of a data\n",
       "augmentation policy with desirable properties as an optimization problem, and\n",
       "derive a generic analytic solution. This solution not only subsumes some\n",
       "existing augmentation schemes, but also leads to an extremely simple data\n",
       "augmentation strategy for NMT: randomly replacing words in both the source\n",
       "sentence and the target sentence with other random words from their\n",
       "corresponding vocabularies. We name this method SwitchOut. Experiments on three\n",
       "translation datasets of different scales show that SwitchOut yields consistent\n",
       "improvements of about 0.5 BLEU, achieving better or comparable performances to\n",
       "strong alternatives such as word dropout (Sennrich et al., 2016a). Code to\n",
       "implement this method is included in the appendix.\n",
       "\n",
       "    </td>\n",
       "      <td id=\"T_e5f14_row3_col2\" class=\"data row3 col2\" ><a href=\"https://arxiv.org/abs/1808.07512\" target=\"_blank\">https://arxiv.org/abs/1808.07512</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f14_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_e5f14_row4_col0\" class=\"data row4 col0\" >Benefits of data augmentation for NMT-based text normalization of user-generated content</td>\n",
       "      <td id=\"T_e5f14_row4_col1\" class=\"data row4 col1\" >One of the most persistent characteristics of written user-generated content (UGC) is the use of non-standard words. This characteristic contributes to an increased difficulty to automatically process and analyze UGC. Text normalization is the task of transforming lexical variants to their canonical forms and is often used as a pre-processing step for conventional NLP tasks in order to overcome the performance drop that NLP systems experience when applied to UGC. In this work, we follow a Neural Machine Translation approach to text normalization. To train such an encoder-decoder model, large parallel training corpora of sentence pairs are required. However, obtaining large data sets with UGC and their normalized version is not trivial, especially for languages other than English. In this paper, we explore how to overcome this data bottleneck for Dutch, a low-resource language. We start off with a publicly available tiny parallel Dutch data set comprising three UGC genres and compare two different approaches. The first is to manually normalize and add training data, a money and time-consuming task. The second approach is a set of data augmentation techniques which increase data size by converting existing resources into synthesized non-standard forms. Our results reveal that a combination of both approaches leads to the best results.</td>\n",
       "      <td id=\"T_e5f14_row4_col2\" class=\"data row4 col2\" ><a href=\"https://biblio.ugent.be/publication/8629139\" target=\"_blank\">https://biblio.ugent.be/publication/8629139</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f14_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_e5f14_row5_col0\" class=\"data row5 col0\" >Data augmentation for low‐resource languages NMT guided by constrained sampling</td>\n",
       "      <td id=\"T_e5f14_row5_col1\" class=\"data row5 col1\" >nothing</td>\n",
       "      <td id=\"T_e5f14_row5_col2\" class=\"data row5 col2\" ><a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22616\" target=\"_blank\">https://onlinelibrary.wiley.com/doi/abs/10.1002/int.22616</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f14_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_e5f14_row6_col0\" class=\"data row6 col0\" >Dictionary-based data augmentation for cross-domain neural machine translation</td>\n",
       "      <td id=\"T_e5f14_row6_col1\" class=\"data row6 col1\" >Existing data augmentation approaches for neural machine translation (NMT)\n",
       "have predominantly relied on back-translating in-domain (IND) monolingual\n",
       "corpora. These methods suffer from issues associated with a domain information\n",
       "gap, which leads to translation errors for low frequency and out-of-vocabulary\n",
       "terminology. This paper proposes a dictionary-based data augmentation (DDA)\n",
       "method for cross-domain NMT. DDA synthesizes a domain-specific dictionary with\n",
       "general domain corpora to automatically generate a large-scale pseudo-IND\n",
       "parallel corpus. The generated pseudo-IND data can be used to enhance a general\n",
       "domain trained baseline. The experiments show that the DDA-enhanced NMT models\n",
       "demonstrate consistent significant improvements, outperforming the baseline\n",
       "models by 3.75-11.53 BLEU. The proposed method is also able to further improve\n",
       "the performance of the back-translation based and IND-finetuned NMT models. The\n",
       "improvement is associated with the enhanced domain coverage produced by DDA.\n",
       "\n",
       "    </td>\n",
       "      <td id=\"T_e5f14_row6_col2\" class=\"data row6 col2\" ><a href=\"https://arxiv.org/abs/2004.02577\" target=\"_blank\">https://arxiv.org/abs/2004.02577</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f14_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_e5f14_row7_col0\" class=\"data row7 col0\" >Soft contextual data augmentation for neural machine translation</td>\n",
       "      <td id=\"T_e5f14_row7_col1\" class=\"data row7 col1\" >While data augmentation is an important trick to boost the accuracy of deep\n",
       "learning methods in computer vision tasks, its study in natural language tasks\n",
       "is still very limited. In this paper, we present a novel data augmentation\n",
       "method for neural machine translation. Different from previous augmentation\n",
       "methods that randomly drop, swap or replace words with other words in a\n",
       "sentence, we softly augment a randomly chosen word in a sentence by its\n",
       "contextual mixture of multiple related words. More accurately, we replace the\n",
       "one-hot representation of a word by a distribution (provided by a language\n",
       "model) over the vocabulary, i.e., replacing the embedding of this word by a\n",
       "weighted combination of multiple semantically similar words. Since the weights\n",
       "of those words depend on the contextual information of the word to be replaced,\n",
       "the newly generated sentences capture much richer information than previous\n",
       "augmentation methods. Experimental results on both small scale and large scale\n",
       "machine translation datasets demonstrate the superiority of our method over\n",
       "strong baselines.\n",
       "\n",
       "    </td>\n",
       "      <td id=\"T_e5f14_row7_col2\" class=\"data row7 col2\" ><a href=\"https://arxiv.org/abs/1905.10523\" target=\"_blank\">https://arxiv.org/abs/1905.10523</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f14_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_e5f14_row8_col0\" class=\"data row8 col0\" >Multi-source neural machine translation with data augmentation</td>\n",
       "      <td id=\"T_e5f14_row8_col1\" class=\"data row8 col1\" >Multi-source translation systems translate from multiple languages to a\n",
       "single target language. By using information from these multiple sources, these\n",
       "systems achieve large gains in accuracy. To train these systems, it is\n",
       "necessary to have corpora with parallel text in multiple sources and the target\n",
       "language. However, these corpora are rarely complete in practice due to the\n",
       "difficulty of providing human translations in all of the relevant languages. In\n",
       "this paper, we propose a data augmentation approach to fill such incomplete\n",
       "parts using multi-source neural machine translation (NMT). In our experiments,\n",
       "results varied over different language combinations but significant gains were\n",
       "observed when using a source language similar to the target language.\n",
       "\n",
       "    </td>\n",
       "      <td id=\"T_e5f14_row8_col2\" class=\"data row8 col2\" ><a href=\"https://arxiv.org/abs/1810.06826\" target=\"_blank\">https://arxiv.org/abs/1810.06826</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5f14_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_e5f14_row9_col0\" class=\"data row9 col0\" >Improving neural machine translation robustness via data augmentation: Beyond back translation</td>\n",
       "      <td id=\"T_e5f14_row9_col1\" class=\"data row9 col1\" >Neural Machine Translation (NMT) models have been proved strong when\n",
       "translating clean texts, but they are very sensitive to noise in the input.\n",
       "Improving NMT models robustness can be seen as a form of \"domain\" adaption to\n",
       "noise. The recently created Machine Translation on Noisy Text task corpus\n",
       "provides noisy-clean parallel data for a few language pairs, but this data is\n",
       "very limited in size and diversity. The state-of-the-art approaches are heavily\n",
       "dependent on large volumes of back-translated data. This paper has two main\n",
       "contributions: Firstly, we propose new data augmentation methods to extend\n",
       "limited noisy data and further improve NMT robustness to noise while keeping\n",
       "the models small. Secondly, we explore the effect of utilizing noise from\n",
       "external data in the form of speech transcripts and show that it could help\n",
       "robustness.\n",
       "\n",
       "    </td>\n",
       "      <td id=\"T_e5f14_row9_col2\" class=\"data row9 col2\" ><a href=\"https://arxiv.org/abs/1910.03009\" target=\"_blank\">https://arxiv.org/abs/1910.03009</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19e89306fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9883a7fd-3c80-4ab7-8fcf-571d391cd84c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
